{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Classifier with Synthetic Training Data\n",
    "\n",
    "Following up on the \"Supervised simple classifier test\" notebook, this notebook tests a \"simple classifier\" model that is supervised on prelabeled synthetic Rainfall data for OCaml.  \n",
    "\n",
    "Prelabeled synthetic Rainfall data is generated with probabilistic grammars that output the distribution of solutions to the Rainfall problem in OCaml. This generative model was first explored in [1], where they explored the use of prograbilistic grammars to generate programs that could then be used to train neural networks on several prelabeled students solutions, and were then used to give students feedback. \n",
    "\n",
    "In this notebook, we take advantage of this generative model in order to explore how our model behaves with respect to data. Specifically, we hope to understand if the model has similar accuracy when trained on synthetic data and, if so, we want to understand how the model's accuracy change when given more synthetic training data.  \n",
    "\n",
    "First we set up the packages we'll use, as well as the local repositories of student data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoplan.trainer import ClassifierTrainer\n",
    "from autoplan.dataset import PrelabeledDataset, build_synthetic_dataset, build_prelabeled_dataset\n",
    "from autoplan.generator import ProgramGenerator\n",
    "from autoplan.vis import plot_accuracy, plot_cm, plot_loss\n",
    "from autoplan.token import PyretTokenizer, OCamlTokenizer\n",
    "from scripts.rainfall_ingest import ingest_dataset\n",
    "\n",
    "from grammars.rainfall.ocaml import Program\n",
    "from grammars.rainfall.labels import GeneralRainfallLabels\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from pprint import pprint\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda:0')\n",
    "REPO_DIR = os.path.expanduser('~/autoplan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Dataset\n",
    "\n",
    "In this section, we specify and ingest the dataset of students' OCaml solutions. Student solutions are preprocessed (as explained on the \"Supervised simple classifier ablation and parameter sweep\" notebook) and tokenized such that all identifies (strings) are normalized. \n",
    "\n",
    "We will use students solutions to evaluate the model accuracy in labeling student-generated data, despite being trained on synthetic data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T1 refers to the OCaml data\n",
    "dataset_name = 'T1'\n",
    "student_dataset = ingest_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let rec rainfall (alof : float list) =\n",
      "  (let rec sum_helper (alof : float list) (sum : float) (counter : float) =\n",
      "     (match (alof, counter) with\n",
      "      | ([],0.) -> failwith \"Empty list.\"\n",
      "      | ((-999.)::_,0.) -> failwith \"Empty list.\"\n",
      "      | ([],_) -> (sum, counter)\n",
      "      | ((-999.)::_,_) -> (sum, counter)\n",
      "      | (hd::tl,_) ->\n",
      "          if hd >= 0.\n",
      "          then sum_helper tl (sum +. hd) (counter +. 1.)\n",
      "          else sum_helper tl sum counter : (float* float)) in\n",
      "   match sum_helper alof 0. 0. with | (s,c) -> s /. c : float)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell optionally to see a sample of a student program\n",
    "print(student_dataset.dataset[0]['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset\n",
    "\n",
    "In this section, we specify and generate the dataset of synthetic OCaml solutions. We specify the number of samples we wish to generate, the grammar we would like to generate samples from, the labels that are used to classify these samples, as well as adaptive and unique sampling -- two hyperparameters that can be set by the user to influence dataset diversity.\n",
    "\n",
    "Adaptive sampling is a sampling technique first described in [1], where dominant choices are dynamically penalized, such that rare programs are more likely to be generated within the same production batch. Unique sampling simply forces the generator to generate unique samples. \n",
    "\n",
    "See the grammar in [ocaml.py](https://github.com/willcrichton/autoplan/blob/master/grammars/rainfall/ocaml.py) for the source code. We will use synthetic solutions generated by this grammar to train and validate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating programs...\n",
      "Generated 345 unique programs.\n",
      "Tokenizing programs...\n",
      "Building dataset metadata...\n"
     ]
    }
   ],
   "source": [
    "N_samples = 500\n",
    "synthetic_dataset = build_synthetic_dataset(\n",
    "    GeneralRainfallLabels,\n",
    "    N=N_samples,\n",
    "    tokenizer=OCamlTokenizer(),\n",
    "    generator=ProgramGenerator(grammar=Program(), adaptive=True),\n",
    "    vocab_index=student_dataset.vocab_index,\n",
    "    unique=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let rainfall list_name =\n",
      "  let helper_name (list_name : int list) =\n",
      "    (match list_name with\n",
      "     | [] -> []\n",
      "     | head::tail when head = (-999) -> []\n",
      "     | head::tail when head < 0 -> helper_name tail\n",
      "     | head::tail when head >= 0 -> head :: (addition_helper_name tail) : \n",
      "    int list) in\n",
      "  if (List.length (helper_name list_name)) = 0\n",
      "  then 0\n",
      "  else\n",
      "    (List.fold_right (+) helper_name list_name 0) /\n",
      "      (List.length (helper_name list_name))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run this cell optionally to see a sample program generated from our grammar\n",
    "print(synthetic_dataset.dataset[1]['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model \n",
    "\n",
    "In this section, we specify which classification model we are going to use as well as the hyperparameters configuration. Based on our previous ablation study, we choose GRU or LSTM, and a size of 512 for both the hidden layer and the embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_options = {\n",
    "    'model' : nn.GRU,\n",
    "    'hidden_size' : 512,\n",
    "    'embedding_size' : 512\n",
    "}\n",
    "\n",
    "trainer = ClassifierTrainer(synthetic_dataset, device, model_opts=model_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997131664cd5442b926936a998549f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: add student accuracy measurement? \n",
    "\n",
    "losses = []\n",
    "train_eval = []\n",
    "val_eval = []\n",
    "student_eval = []\n",
    "\n",
    "for _ in tqdm(range(50)):\n",
    "    losses.append(trainer.train_one_epoch())\n",
    "    train, val = trainer.eval()\n",
    "    train_eval.append(train)\n",
    "    val_eval.append(val)\n",
    "    \n",
    "    trainer.model.eval()\n",
    "    student_eval.append(trainer.eval_on(student_dataset.loader(student_dataset.dataset)))\n",
    "    trainer.model.train() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "In this section we analyze the performance of our model. First we look how loss varied at each epoch. Loss refers to the distance between the predicted label and the true label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss over epoch\n",
    "plot_loss(losses, title='Classifier loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we look at the model accuracy at each epoch, as evaluated on the training set and on the evaluation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy over epoch as measured on the training set\n",
    "plot_accuracy(train_eval, title='Training set accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy over epoch as measured on the validation set\n",
    "plot_accuracy(val_eval, title='Validation set accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy over epoch as measured on the validation set\n",
    "plot_accuracy(student_eval, title='Student dataset accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we look at the performance of the fully trained model on student data as shown by the confusion matrix. Specifically, we look at which fraction of the true labels our model correctly predicted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy at the end of the training loop, measured on the student dataset\n",
    "trainer.model.eval()\n",
    "trainer.eval_on(student_dataset.loader(student_dataset.dataset)).plot_cm(\"Confusion Matrix\", normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model reached perfect accuracy at learning to classify the \"Clear Multiple\" label, similar accuracy to the post-supervised classifier at classifying the \"Single Loop\" label, and promising accuracy at classifying the \"Clean First\" label. \n",
    "\n",
    "Our next step is to run an ablation test and parameter swap on this model in order to understand how each hyperparameter and dataset design choice (adaptive, unique) impacts the final performance, as well as how the size of the dataset affects classification accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. M. Wu, M. Mosse, N. Goodman, and C. Piech. Zero shot learning for code education: Rubric sampling with deep learning inference. arXiv preprint arXiv:1809.01357, 2018."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
